{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sys"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function which starts the driver in non-bot mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_driver():\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.headless = False\n",
    "\n",
    "        # Disabling chrome's bot mode\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_experimental_option(\n",
    "            \"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "        prefs = {\n",
    "            \"translate_whitelists\": {\"uk\": \"en\"},\n",
    "            \"translate\": {\"enabled\": \"True\"}\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        options.add_argument(\n",
    "            f'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--window-size=1420, 1200')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        \n",
    "        driver = webdriver.Chrome(\n",
    "            executable_path='./chromedriver',\n",
    "            options=options\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Failed to run webdriver(check webdriver version and if it\\' in the same dir with script\\n\\n)')\n",
    "        sys.exit(1)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_15728\\245943858.py:192: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n",
      "  options.headless = False\n",
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_15728\\245943858.py:211: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\n"
     ]
    }
   ],
   "source": [
    "driver = start_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_day_srting(today: str) -> str:\n",
    "    '''\n",
    "    This function calculates the string representation of the next day given a starting date\n",
    "    ---\n",
    "    today: str - the starting date in the format YYYY-MM-DD\n",
    "    returns: str - the next day in the format YYYY-MM-DD\n",
    "    '''\n",
    "    date_object = datetime.strptime(today, '%Y-%m-%d')\n",
    "    next_day_object = date_object + timedelta(days=1)\n",
    "    next_day_string = next_day_object.strftime('%Y-%m-%d')\n",
    "\n",
    "    return next_day_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_url_by_date(start_date: str) -> str:\n",
    "    '''\n",
    "    This function creates a search URL for Twitter given a starting date\n",
    "    ---\n",
    "    start_date: str - the starting date to search for tweets in the format YYYY-MM-DD\n",
    "    returns: str - the URL for the Twitter search\n",
    "    '''\n",
    "    end_date = next_day_srting(start_date)\n",
    "    # url = f'https://twitter.com/search?f=live&q=(%23bitcoin)%20lang%3Aen%20until%3A{end_date}%20since%3A{start_date}%20-filter%3Areplies&src=typed_query'\n",
    "    # url = f\"https://twitter.com/search?q=(%23bitcoin)%20lang%3Aen%20until%3A{end_date}%20since%3A{start_date}%20-filter%3Areplies&src=typeahead_click\"\n",
    "    # url = f'https://twitter.com/search?q=(%23bitcoin)%20min_replies%3A10%20min_retweets%3A10%20lang%3Aen%20until%3A{end_date}%20since%3A{start_date}&src=typed_query&f=live'\n",
    "    url = f'https://twitter.com/search?q=%23bitcoin%20min_retweets%3A10%20lang%3Aen%20until%3A{end_date}%20since%3A{start_date}&src=typed_query&f=live'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://twitter.com/search?q=%23bitcoin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_input = driver.find_element(By.XPATH, '//*[@id=\"layers\"]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div[5]/label/div/div[2]/div/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "username_input.send_keys('JohnaValeriia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_button = driver.find_element(By.XPATH, '//*[@id=\"layers\"]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div[6]/div/span/span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "password_input = driver.find_element(By.XPATH, '//*[@id=\"layers\"]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div[1]/div/div/div[3]/div/label/div/div[2]/div[1]/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "password_input.send_keys('rb2MSl15YILy7O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_button = driver.find_element(By.XPATH, '//*[@id=\"layers\"]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div/div[1]/div/div/div/div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_button.click()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to authenticate into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate():\n",
    "    username_input = driver.find_element(By.XPATH, '//*[@id=\"layers\"]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div[5]/label/div/div[2]/div/input')\n",
    "    username_input.send_keys('JohnaValeriia')\n",
    "    \n",
    "    next_button = driver.find_element(By.XPATH, '//*[@id=\"layers\"]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div/div/div/div[6]/div/span/span')\n",
    "    next_button.click()\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    password_input = driver.find_element(By.XPATH, '//*[@id=\"layers\"]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div[1]/div/div/div[3]/div/label/div/div[2]/div[1]/input')\n",
    "    password_input.send_keys('rb2MSl15YILy7O')\n",
    "    \n",
    "    login_button = driver.find_element(By.XPATH, '//*[@id=\"layers\"]/div/div/div/div/div/div/div[2]/div[2]/div/div/div[2]/div[2]/div[2]/div/div[1]/div/div/div/div')\n",
    "    login_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import threading\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from csv import DictWriter\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "import sys\n",
    "\n",
    "LOCK = threading.Lock()\n",
    "\n",
    "\n",
    "def scroll_to_bottom(driver: webdriver.Chrome):\n",
    "    '''\n",
    "    This function scrolls down to the bottom of the webpage\n",
    "    ---\n",
    "    driver: webdriver.Chrome - the Chrome driver object to control the browser\n",
    "    '''\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "def get_body_heigth(driver: webdriver.Chrome) -> int:\n",
    "    '''\n",
    "    This function returns the height of the body of the webpage\n",
    "    ---\n",
    "    driver: webdriver.Chrome - the Chrome driver object to control the browser\n",
    "    returns: int - the height of the webpage's body\n",
    "    '''\n",
    "    return driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "def _scrap(driver: webdriver.Chrome) -> list[list[str]]:\n",
    "    '''\n",
    "    This is an internal function that scrapes tweet data from Twitter\n",
    "    ---\n",
    "    driver: webdriver.Chrome - the Chrome driver object to control the browser\n",
    "    returns: list[list[str]] - a list of lists containing the scraped tweet data\n",
    "    '''\n",
    "    js_scrap_script = ''' const all_data = [];\n",
    "        const all_tweets = document.getElementsByTagName(\"section\")[0].childNodes[1].childNodes[0];\n",
    "\n",
    "        for (let i = 0; i < all_tweets.childElementCount; i++) {\n",
    "            const single_tweet_data = []\n",
    "            const text = document.getElementsByTagName(\"section\")[0].childNodes[1].childNodes[0].childNodes[i].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[1].childNodes[1].childNodes[0].textContent;\n",
    "            const user_name = document.getElementsByTagName(\"section\")[0].childNodes[1].childNodes[0].childNodes[i].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[1].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[0].childNodes[0].textContent;\n",
    "            let date = document.getElementsByTagName(\"section\")[0].childNodes[1].childNodes[0].childNodes[i].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[1].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[0].childNodes[2].childNodes[0].childNodes[0].getAttribute(\"datetime\");\n",
    "            if (date === null) {\n",
    "            date = document.getElementsByTagName(\"section\")[0].childNodes[1].childNodes[0].childNodes[i].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[1].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[0].childNodes[2].childNodes[0].childNodes[0].childNodes[0].getAttribute(\"datetime\");\n",
    "        }\n",
    "            let url_with_id = document.getElementsByTagName(\"section\")[0].childNodes[1].childNodes[0].childNodes[i].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[1].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[0].childNodes[2].childNodes[0].getAttribute(\"href\");\n",
    "            if (url_with_id === null) {\n",
    "            url_with_id = document.getElementsByTagName(\"section\")[0].childNodes[1].childNodes[0].childNodes[i].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[1].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[0].childNodes[1].childNodes[0].childNodes[2].childNodes[0].childNodes[0].getAttribute(\"href\");\n",
    "        }\n",
    "            const verif_path = document.getElementsByTagName(\"section\")[0].childNodes[1].childNodes[0].childNodes[i].getElementsByTagName(\"svg\")[0].childNodes[0].childNodes[0].getAttribute(\"d\");\n",
    "            \n",
    "                single_tweet_data.push(text);\n",
    "                single_tweet_data.push(user_name);\n",
    "                single_tweet_data.push(date);\n",
    "                single_tweet_data.push(url_with_id);\n",
    "                single_tweet_data.push(verif_path);\n",
    "                all_data.push(single_tweet_data); \n",
    "        }\n",
    "        return all_data; '''\n",
    "    try:\n",
    "        tweets_on_page = driver.execute_script(js_scrap_script)\n",
    "    except:\n",
    "        tweets_on_page = []\n",
    "    return tweets_on_page\n",
    "\n",
    "def transfom_date_time(date_time: str) -> str:\n",
    "    '''\n",
    "    This function transforms a date-time string into a formatted date string\n",
    "    ---\n",
    "    date_time: str - the date-time string to transform\n",
    "    returns: str - the formatted date string\n",
    "    '''\n",
    "    date_object = datetime.fromisoformat(date_time.replace('Z', '+00:00'))\n",
    "    formatted_date = date_object.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    return formatted_date\n",
    "\n",
    "def update_csv(data: list[list[str]]):\n",
    "    '''\n",
    "    This function updates a CSV file with scraped tweet data\n",
    "    ---\n",
    "    data: list[list[str]] - a list of lists containing the scraped tweet data\n",
    "    '''\n",
    "    verif_checkmark_path = 'M20.396 11c-.018-.646-.215-1.275-.57-1.816-.354-.54-.852-.972-1.438-1.246.223-.607.27-1.264.14-1.897-.131-.634-.437-1.218-.882-1.687-.47-.445-1.053-.75-1.687-.882-.633-.13-1.29-.083-1.897.14-.273-.587-.704-1.086-1.245-1.44S11.647 1.62 11 1.604c-.646.017-1.273.213-1.813.568s-.969.854-1.24 1.44c-.608-.223-1.267-.272-1.902-.14-.635.13-1.22.436-1.69.882-.445.47-.749 1.055-.878 1.688-.13.633-.08 1.29.144 1.896-.587.274-1.087.705-1.443 1.245-.356.54-.555 1.17-.574 1.817.02.647.218 1.276.574 1.817.356.54.856.972 1.443 1.245-.224.606-.274 1.263-.144 1.896.13.634.433 1.218.877 1.688.47.443 1.054.747 1.687.878.633.132 1.29.084 1.897-.136.274.586.705 1.084 1.246 1.439.54.354 1.17.551 1.816.569.647-.016 1.276-.213 1.817-.567s.972-.854 1.245-1.44c.604.239 1.266.296 1.903.164.636-.132 1.22-.447 1.68-.907.46-.46.776-1.044.908-1.681s.075-1.299-.165-1.903c.586-.274 1.084-.705 1.439-1.246.354-.54.551-1.17.569-1.816zM9.662 14.85l-3.429-3.428 1.293-1.302 2.072 2.072 4.4-4.794 1.347 1.246z'\n",
    "    \n",
    "    global LOCK\n",
    "    LOCK.acquire()\n",
    "    with open(\"twitterData.csv\", \"a+\", encoding='utf-8', newline='') as file:\n",
    "        fieldnames = ['Time','Text', 'Name', 'Tweet_id', 'Verified']\n",
    "        writer = DictWriter(file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for tweet in data:\n",
    "            writer.writerow({\n",
    "                'Time': transfom_date_time(tweet[2]),\n",
    "                'Text': tweet[0].replace('\\n', ' '),\n",
    "                'Name': tweet[1],\n",
    "                'Tweet_id': tweet[3],\n",
    "                'Verified': tweet[4] == verif_checkmark_path\n",
    "            })\n",
    "    LOCK.release()\n",
    "\n",
    "\n",
    "def get_url_by_date(start_date: str) -> str:\n",
    "    '''\n",
    "    This function creates a search URL for Twitter given a starting date\n",
    "    ---\n",
    "    start_date: str - the starting date to search for tweets in the format YYYY-MM-DD\n",
    "    returns: str - the URL for the Twitter search\n",
    "    '''\n",
    "    end_date = next_day_srting(start_date)\n",
    "    # url = f'https://twitter.com/search?f=live&q=(%23bitcoin)%20lang%3Aen%20until%3A{end_date}%20since%3A{start_date}%20-filter%3Areplies&src=typed_query'\n",
    "    # url = f\"https://twitter.com/search?q=(%23bitcoin)%20lang%3Aen%20until%3A{end_date}%20since%3A{start_date}%20-filter%3Areplies&src=typeahead_click\"\n",
    "    # url = f'https://twitter.com/search?q=(%23bitcoin)%20min_replies%3A10%20min_retweets%3A10%20lang%3Aen%20until%3A{end_date}%20since%3A{start_date}&src=typed_query&f=live'\n",
    "    url = f'https://twitter.com/search?q=%23bitcoin%20min_retweets%3A10%20lang%3Aen%20until%3A{end_date}%20since%3A{start_date}&src=typed_query&f=live'\n",
    "    \n",
    "    return url\n",
    "\n",
    "def get_date_to_scrap() -> list[str]:\n",
    "    '''\n",
    "    This function gets a list of dates to scrape from a file of missing hours\n",
    "    ---\n",
    "    returns: list[str] - a list of dates to scrape\n",
    "    '''\n",
    "    with open('missing_hours.csv', 'r') as f:\n",
    "        dates = set()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            dates.add(line.split(' ')[0])\n",
    "    \n",
    "    return list(dates)\n",
    "\n",
    "def next_day_srting(today: str) -> str:\n",
    "    '''\n",
    "    This function calculates the string representation of the next day given a starting date\n",
    "    ---\n",
    "    today: str - the starting date in the format YYYY-MM-DD\n",
    "    returns: str - the next day in the format YYYY-MM-DD\n",
    "    '''\n",
    "    date_object = datetime.strptime(today, '%Y-%m-%d')\n",
    "    next_day_object = date_object + timedelta(days=1)\n",
    "    next_day_string = next_day_object.strftime('%Y-%m-%d')\n",
    "\n",
    "    return next_day_string\n",
    "\n",
    "def scrap(driver: webdriver.Chrome, time_to_scrap: int):\n",
    "    '''\n",
    "    This function scrapes Twitter for a given amount of time and saves the tweets to a CSV file\n",
    "    ---\n",
    "    driver: webdriver.Chrome - the Chrome driver object used for scraping\n",
    "    time_to_scrap: int - the number of seconds to scrape Twitter for\n",
    "    '''\n",
    "    all_tweets_to_save = []\n",
    "    buffer = []\n",
    "\n",
    "    current_height = get_body_heigth(driver)\n",
    "    last_height = 0\n",
    "    counter = 0\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    while(time.time() - start_time < time_to_scrap):\n",
    "        time.sleep(.5)\n",
    "        current_tweets = _scrap(driver)\n",
    "        tweets_to_save = [tweet for tweet in current_tweets if tweet not in buffer]\n",
    "        if current_tweets:\n",
    "            buffer = current_tweets\n",
    "        all_tweets_to_save.extend(tweets_to_save)\n",
    "        scroll_to_bottom(driver)\n",
    "\n",
    "        last_height = current_height\n",
    "        current_height = get_body_heigth(driver)\n",
    "        if last_height == current_height:\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "\n",
    "        if counter > 10:\n",
    "            break\n",
    "\n",
    "    update_csv(all_tweets_to_save)\n",
    "\n",
    "\n",
    "def start_driver():\n",
    "    try:\n",
    "        options = Options()\n",
    "        options.headless = False\n",
    "\n",
    "        # Disabling chrome's bot mode\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_experimental_option(\n",
    "            \"excludeSwitches\", [\"enable-automation\"])\n",
    "        options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "        prefs = {\n",
    "            \"translate_whitelists\": {\"uk\": \"en\"},\n",
    "            \"translate\": {\"enabled\": \"True\"}\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "        options.add_argument(\n",
    "            f'user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--window-size=1420, 1200')\n",
    "        options.add_argument('--disable-gpu')\n",
    "        \n",
    "        driver = webdriver.Chrome(\n",
    "            executable_path='./chromedriver',\n",
    "            options=options\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print('Failed to run webdriver(check webdriver version and if it\\' in the same dir with script\\n\\n)')\n",
    "        sys.exit(1)\n",
    "\n",
    "    return driver\n",
    "\n",
    "def multithred_func(day: str):\n",
    "    '''\n",
    "    This function scrapes Twitter for a single day in a separate thread\n",
    "    ---\n",
    "    day: str - the date to scrape tweets for, in format 'YYYY-MM-DD'\n",
    "    '''\n",
    "    with open('scraped_date.json', 'r') as f:\n",
    "        already_scraped_days: list = json.load(f)\n",
    "\n",
    "    if day in already_scraped_days:\n",
    "        return\n",
    "\n",
    "    url_to_scrap = get_url_by_date(day)\n",
    "\n",
    "    global LOCK\n",
    "\n",
    "    LOCK.acquire()\n",
    "    # driver = start_driver()\n",
    "    global driver\n",
    "    time.sleep(5 + random.random())\n",
    "    LOCK.release()\n",
    "\n",
    "\n",
    "    driver.get(url_to_scrap)\n",
    "    \n",
    "    time.sleep(10)\n",
    "    if not driver.current_url.split('/')[3].startswith('search'):\n",
    "        authenticate()\n",
    "    driver.get(url_to_scrap)\n",
    "\n",
    "    scrap(driver, 60*10)\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "\n",
    "    LOCK.acquire()\n",
    "    with open('scraped_date.json', 'r') as f:\n",
    "        already_scraped_days: list = json.load(f)\n",
    "\n",
    "    already_scraped_days.append(day)\n",
    "\n",
    "    with open('scraped_date.json', 'w') as f:\n",
    "        json.dump(already_scraped_days, f)\n",
    "    LOCK.release()\n",
    "\n",
    "    print(f'already scraped days:  {len(already_scraped_days)}')\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    dates = get_date_to_scrap()\n",
    "\n",
    "    print(f'number of days to scrap: {len(dates)}')\n",
    "\n",
    "    with ThreadPoolExecutor(4) as executor:\n",
    "        executor.map(multithred_func, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = get_date_to_scrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2022-09-26',\n",
       " '2022-07-26',\n",
       " '2022-07-09',\n",
       " '2022-06-10',\n",
       " '2022-05-25',\n",
       " '2022-11-24',\n",
       " '2022-02-19',\n",
       " '2022-12-06',\n",
       " '2022-04-21',\n",
       " '2022-07-03',\n",
       " '2022-10-30',\n",
       " '2022-04-23',\n",
       " '2022-05-05',\n",
       " '2022-10-22',\n",
       " '2022-09-25',\n",
       " '2022-04-04',\n",
       " '2022-05-06',\n",
       " '2022-10-06',\n",
       " '2022-08-05',\n",
       " '2022-06-16',\n",
       " '2022-04-09',\n",
       " '2022-02-26',\n",
       " '2022-04-24',\n",
       " '2022-10-24',\n",
       " '2022-05-11',\n",
       " '2022-07-19',\n",
       " '2022-10-01',\n",
       " '2022-09-27',\n",
       " '2022-04-22',\n",
       " '2022-02-09',\n",
       " '2022-04-08',\n",
       " '2022-06-08',\n",
       " '2022-09-21',\n",
       " '2022-12-08',\n",
       " '2022-02-14',\n",
       " '2022-03-16',\n",
       " '2022-03-05',\n",
       " '2022-11-02',\n",
       " '2022-05-10',\n",
       " '2022-05-23',\n",
       " '2022-10-27',\n",
       " '2022-11-01',\n",
       " '2022-09-11',\n",
       " '2022-11-17',\n",
       " '2022-05-03',\n",
       " '2022-01-16',\n",
       " '2022-04-17',\n",
       " '2022-05-20',\n",
       " '2022-01-23',\n",
       " '2022-06-29',\n",
       " '2022-05-02',\n",
       " '2022-07-24',\n",
       " '2022-07-22',\n",
       " '2022-06-14',\n",
       " '2022-02-13',\n",
       " '2022-02-10',\n",
       " '2022-12-09',\n",
       " '2022-01-04',\n",
       " '2022-08-06',\n",
       " '2022-11-30',\n",
       " '2022-09-10',\n",
       " '2022-11-14',\n",
       " '2022-11-28',\n",
       " '2022-03-22',\n",
       " '2022-01-06',\n",
       " '2022-12-02',\n",
       " '2022-07-10',\n",
       " '2022-09-20',\n",
       " '2022-10-31',\n",
       " '2022-09-09',\n",
       " '2022-08-18',\n",
       " '2022-12-28',\n",
       " '2022-03-01',\n",
       " '2022-08-11',\n",
       " '2022-08-08',\n",
       " '2022-02-21',\n",
       " '2022-11-29',\n",
       " '2022-09-04',\n",
       " '2022-03-17',\n",
       " '2022-05-09',\n",
       " '2022-05-22',\n",
       " '2022-08-30',\n",
       " '2022-01-18',\n",
       " '2022-07-17',\n",
       " '2022-12-10',\n",
       " '2022-05-21',\n",
       " '2022-04-10',\n",
       " '2022-03-04',\n",
       " '2022-03-06',\n",
       " '2022-02-04',\n",
       " '2022-10-05',\n",
       " '2022-07-30',\n",
       " '2022-06-05',\n",
       " '2022-10-20',\n",
       " '2022-07-02',\n",
       " '2023-01-02',\n",
       " '2022-08-14',\n",
       " '2022-01-19',\n",
       " '2022-02-12',\n",
       " '2022-11-08',\n",
       " '2022-09-15',\n",
       " '2022-06-11',\n",
       " '2022-03-25',\n",
       " '2022-05-13',\n",
       " '2022-09-13',\n",
       " '2022-08-13',\n",
       " '2022-12-14',\n",
       " '2022-01-29',\n",
       " '2022-06-12',\n",
       " '2022-06-19',\n",
       " '2023-01-04',\n",
       " '2022-11-18',\n",
       " '2022-08-29',\n",
       " '2022-12-12',\n",
       " '2022-12-04',\n",
       " '2022-07-06',\n",
       " '2022-06-25',\n",
       " '2022-10-29',\n",
       " '2022-09-08',\n",
       " '2022-11-23',\n",
       " '2023-01-03',\n",
       " '2022-11-16',\n",
       " '2022-09-18',\n",
       " '2022-03-19',\n",
       " '2022-11-09',\n",
       " '2022-06-07',\n",
       " '2022-02-27',\n",
       " '2022-01-14',\n",
       " '2022-05-08',\n",
       " '2022-11-26',\n",
       " '2022-09-28',\n",
       " '2022-02-01',\n",
       " '2022-08-01',\n",
       " '2022-02-28',\n",
       " '2022-09-19',\n",
       " '2022-10-02',\n",
       " '2022-07-21',\n",
       " '2022-05-19',\n",
       " '2022-01-17',\n",
       " '2022-02-22',\n",
       " '2022-08-07',\n",
       " '2022-11-04',\n",
       " '2022-06-21',\n",
       " '2022-12-29',\n",
       " '2022-08-20',\n",
       " '2022-12-21',\n",
       " '2022-10-08',\n",
       " '2022-06-22',\n",
       " '2022-07-20',\n",
       " '2022-05-24',\n",
       " '2022-12-18',\n",
       " '2022-02-07',\n",
       " '2022-06-06',\n",
       " '2022-07-23',\n",
       " '2022-03-11',\n",
       " '2022-01-24',\n",
       " '2022-08-17',\n",
       " '2022-12-26',\n",
       " '2022-03-12',\n",
       " '2022-03-18',\n",
       " '2022-08-31',\n",
       " '2022-12-30',\n",
       " '2022-06-27',\n",
       " '2022-06-03',\n",
       " '2022-02-02',\n",
       " '2022-01-03',\n",
       " '2023-01-06',\n",
       " '2022-11-05',\n",
       " '2022-06-09',\n",
       " '2022-11-22',\n",
       " '2022-10-26',\n",
       " '2022-06-20',\n",
       " '2022-07-18',\n",
       " 'date',\n",
       " '2022-03-08',\n",
       " '2022-01-02',\n",
       " '2022-07-11',\n",
       " '2022-10-07',\n",
       " '2022-05-04',\n",
       " '2022-09-16',\n",
       " '2022-09-22',\n",
       " '2022-05-18',\n",
       " '2022-08-22',\n",
       " '2022-09-24',\n",
       " '2022-04-12',\n",
       " '2022-12-01',\n",
       " '2022-07-07',\n",
       " '2022-04-06',\n",
       " '2022-01-15',\n",
       " '2022-01-07',\n",
       " '2022-03-29',\n",
       " '2023-01-01',\n",
       " '2022-10-23',\n",
       " '2022-10-18',\n",
       " '2022-02-06',\n",
       " '2022-03-27',\n",
       " '2022-11-11',\n",
       " '2022-11-25',\n",
       " '2022-01-01',\n",
       " '2022-04-27',\n",
       " '2022-03-30',\n",
       " '2022-09-06',\n",
       " '2022-10-09',\n",
       " '2022-11-20',\n",
       " '2022-07-05',\n",
       " '2022-04-11',\n",
       " '2022-08-02',\n",
       " '2022-10-16',\n",
       " '2022-12-20',\n",
       " '2022-04-07',\n",
       " '2022-08-27',\n",
       " '2022-09-17',\n",
       " '2022-03-03',\n",
       " '2022-08-23',\n",
       " '2022-11-10',\n",
       " '2022-02-15',\n",
       " '2022-02-20',\n",
       " '2022-06-24',\n",
       " '2022-08-15',\n",
       " '2022-01-26',\n",
       " '2022-02-05',\n",
       " '2022-08-25',\n",
       " '2022-05-16',\n",
       " '2022-12-17',\n",
       " '2022-07-29',\n",
       " '2022-07-25',\n",
       " '2022-01-12',\n",
       " '2022-11-19',\n",
       " '2022-03-14',\n",
       " '2022-10-03',\n",
       " '2022-05-28',\n",
       " '2022-12-24',\n",
       " '2022-04-26',\n",
       " '2022-09-23',\n",
       " '2022-04-25',\n",
       " '2022-04-02',\n",
       " '2022-12-05',\n",
       " '2022-03-20',\n",
       " '2022-01-05',\n",
       " '2022-09-29',\n",
       " '2022-11-15',\n",
       " '2022-11-12',\n",
       " '2022-06-13',\n",
       " '2022-06-02',\n",
       " '2022-02-17',\n",
       " '2022-04-29',\n",
       " '2022-11-07',\n",
       " '2022-02-03',\n",
       " '2022-03-28',\n",
       " '2022-03-09',\n",
       " '2022-01-27',\n",
       " '2022-06-30',\n",
       " '2022-12-03',\n",
       " '2022-01-11',\n",
       " '2022-01-09',\n",
       " '2022-10-13',\n",
       " '2022-08-10',\n",
       " '2022-12-25',\n",
       " '2022-09-30',\n",
       " '2022-12-15',\n",
       " '2022-01-31',\n",
       " '2022-04-20',\n",
       " '2022-09-05',\n",
       " '2022-03-23',\n",
       " '2022-07-04',\n",
       " '2022-07-27',\n",
       " '2022-01-25',\n",
       " '2022-12-19',\n",
       " '2022-08-21',\n",
       " '2022-04-05',\n",
       " '2022-07-28',\n",
       " '2022-08-28',\n",
       " '2022-10-14',\n",
       " '2022-11-13',\n",
       " '2022-02-08',\n",
       " '2022-08-19',\n",
       " '2022-06-28',\n",
       " '2022-12-11',\n",
       " '2022-04-13',\n",
       " '2022-08-09',\n",
       " '2022-10-17',\n",
       " '2022-05-07',\n",
       " '2022-11-03',\n",
       " '2022-01-30',\n",
       " '2022-02-16',\n",
       " '2022-10-15',\n",
       " '2022-08-26',\n",
       " '2022-07-01',\n",
       " '2022-01-10',\n",
       " '2022-11-27',\n",
       " '2022-02-18',\n",
       " '2022-09-02',\n",
       " '2022-08-03',\n",
       " '2022-04-03',\n",
       " '2022-08-16',\n",
       " '2022-10-21',\n",
       " '2022-02-24',\n",
       " '2022-05-15',\n",
       " '2022-08-04',\n",
       " '2022-03-13',\n",
       " '2022-12-07',\n",
       " '2022-09-12',\n",
       " '2022-02-11',\n",
       " '2022-08-12',\n",
       " '2022-04-01',\n",
       " '2022-03-31',\n",
       " '2022-10-25',\n",
       " '2022-03-26',\n",
       " '2022-12-27',\n",
       " '2022-12-31',\n",
       " '2022-06-17',\n",
       " '2022-09-03',\n",
       " '2022-01-28',\n",
       " '2022-05-14',\n",
       " '2022-10-04',\n",
       " '2022-10-19',\n",
       " '2022-07-31',\n",
       " '2022-08-24',\n",
       " '2022-09-07',\n",
       " '2022-06-04',\n",
       " '2022-05-12',\n",
       " '2022-10-10',\n",
       " '2022-05-27',\n",
       " '2022-11-06',\n",
       " '2022-03-02',\n",
       " '2023-01-05',\n",
       " '2022-02-23',\n",
       " '2022-11-21',\n",
       " '2022-12-23',\n",
       " '2022-01-08',\n",
       " '2022-12-22',\n",
       " '2022-05-29',\n",
       " '2022-12-16',\n",
       " '2022-12-13',\n",
       " '2022-07-08',\n",
       " '2022-06-23',\n",
       " '2022-05-17',\n",
       " '2022-02-25',\n",
       " '2022-10-28']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.current_url.split('/')[3].startswith('search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already scraped days:  1\n"
     ]
    }
   ],
   "source": [
    "multithred_func(dates[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
